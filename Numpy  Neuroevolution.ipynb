{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Zero Gradient Challenge: Neuroevolution using only Numpy!\n",
    "#### By Jacob Gursky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "What if I told you that you can train neural networks without ever calculating a gradient, and only using the forward pass?  Such is the magic of **neuroevolution!** Also, I am going to show that all this can easily be done using only Numpy!  This is a bit of an ongoing project that I have been working on off and on for a while now, but let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neuroevolution?\n",
    "\n",
    "First off, for those of you that don't already know, **neuroevolution** describes the application of evolutionary algorithms to training neural networks as a gradient-free alternative!  We are going to use an extremely simple case of neuroevolution here, only using a fixed topology network and focusing on optimizing only weights and biases.  The neuroevolutionary process can be defined by four fundamental steps that are repeated until convergence is reached, starting with a pool of randomly generated networks.\n",
    "\n",
    "1. Evaluate fitness of the population\n",
    "2. Select the most fit individuals to reproduce\n",
    "3. Create offspring using crossover\n",
    "4. Introduce random mutations to children\n",
    "\n",
    "Wow, this seems pretty simple!  Let's break down some of the terminology a bit:\n",
    "\n",
    "- **Fitness**: This simply describes how well the network performed at a certain task and allows us to determine which networks to breed.  Note that because evolutionary algorithms are a form of non-convex optimization, and therefore can be used with any loss function, regardless of its differentiability (of lack thereof)\n",
    "\n",
    "- **Crossover**: This denotes the process by which a child network is created from two parent networks.  Essentially, neurons are randomly sampled from each of the parent networks and placed into the child network.  Note that we are using the entire neuron for crossover. That means that for a given neuron in a child network, every weight and the bias for that one neuron came entirely from one of the parents.\n",
    "\n",
    "- **Mutation**:  This one is probably the easiest!  In order for our child networks to improve, we have to introduce random changes to the network weights, often drawn from a uniform or normal distibution. There can be many different forms of mutation, shift mutations (which multiply the paramters by a random number), swap mutations (which replace the parameter with a random number), and sign mutations (which change the sign of a parameter) are the three that we are going to deal with in this project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Neuroevolution\n",
    "\n",
    "We should also consider the theoretical advantages of neuroevolutionary modeling.  First off, we only need to use the forward pass of the network as we only need to calculate the loss in order to determine which networks to breed.  The implications of this is obvious, the backwards pass is usually the most expensive!  Second, evolutionary algorithms are guarenteed to find the global minimum of a loss surface given enough iterations, whereas convex gradient-based methods can get stuck in local minima.  Lastly, more sophisticated forms of neuroevolution allow us to not only optimize the weights of a network, but also the structure itself!\n",
    "\n",
    "Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries\n",
    "\n",
    "As laid out in the introduction, we are going to try and use only numpy for this project, only defining the helper functions that we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "We first are going to define a few of the helper functions to set up our networks.  First off is the relu activation function, which we will use as the activation function for our hidden layers, and the softmax function for the output of the network to get probabilistic estimates of the network output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our activation function\n",
    "def relu(x):\n",
    "    return np.where(x>0,x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the softmax function\n",
    "def softmax(x):\n",
    "    x = np.exp(x - np.max(x))\n",
    "    return np.array(x / x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Random Network\n",
    "\n",
    "Another key function we need to define is a function to create a random network of a fixed topology to create our initial population of networks.  Essentially we just need to create some randomly initialized matrices according to the network structure given to the functions, so this is relatively simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our random network generation function\n",
    "def create_network(n_units=(128,64),input_shape=784,output_shape=10):\n",
    "    # First we need to randomly initialize our weight and bias matrices\n",
    "    weights = []\n",
    "    biases = []\n",
    "    # Creating the weights for the hidden layers\n",
    "    for i in range(len(n_units)):\n",
    "        if i==0:\n",
    "            weights.append(np.random.uniform(-0.15,0.15,size=(input_shape,n_units[0])).astype('float32'))\n",
    "            biases.append(np.zeros(n_units[0]).astype('float32'))\n",
    "        else:\n",
    "            weights.append(np.random.uniform(-0.15,0.15,size=(n_units[i-1],n_units[i])).astype('float32'))\n",
    "            biases.append(np.zeros(n_units[i]).astype('float32'))\n",
    "    # Creating weights and biases for output layer\n",
    "    weights.append(np.random.uniform(-0.15,0.15,size=(n_units[-1],output_shape)).astype('float32'))\n",
    "    biases.append(np.zeros(output_shape))\n",
    "    return weights+biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding the Data Forwards\n",
    "\n",
    "We also need a function that, given the weight and bias matrices of a network, can propogate given inputs forward!  This follows standard convention for perceptrons, returning probability estimates that we will use to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create our feed forward function\n",
    "def feed_forward(inputs, network):\n",
    "    # Dividing into the weights and biases\n",
    "    weights = network[0:len(network)//2]\n",
    "    biases = network[len(network)//2:]\n",
    "    # First we need to propogate inputs\n",
    "    a = relu((inputs@weights[0])+biases[0])\n",
    "    # Now we need to iterate through all of the remaining elements\n",
    "    for i in range(1,len(weights)):\n",
    "        a = relu((a@weights[i])+biases[i])\n",
    "    # Now we need to run softmax over the result\n",
    "    probs = np.apply_along_axis(softmax, axis=1, arr=a)\n",
    "    # Finally, return the max\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Offspring Networks\n",
    "\n",
    "Another key aspect of neuroevolution is being able to have networks breed!  Therefore, we need a function that, given two networks, will create a child network that has half of the neurons from the two parent networks!  Again, relatively simple code that should be easy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a better offspring generation function that selects neurons rather than random weights\n",
    "def offspring_network(weight_set1, weight_set2):\n",
    "    # First we need to create the child\n",
    "    child = [np.copy(i) for i in weight_set1]\n",
    "    # We need to select random neurons from each layer\n",
    "    # Axis 1 of W represents neurons\n",
    "    biases = [np.copy(i) for i in weight_set2[len(weight_set2)//2:]]\n",
    "    selected_biases = np.array([np.random.choice(range(i.shape[0]),size=i.shape[0]//2) for i in biases])\n",
    "    # Now to need to crossover the neurons in the child network\n",
    "    for i in range(int(np.floor(len(child)/2))):\n",
    "        for j in selected_biases[i]:\n",
    "            child[i][:,j] = weight_set2[i][:,j]\n",
    "    for i in range((len(child)//2)+1, len(child)):\n",
    "        for j in selected_biases[i-(len(child)//2)]:\n",
    "            child[i][j] = weight_set2[i][j]\n",
    "    return child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to mutate a single network\n",
    "def mutate_layer(x, mutation_prob=0.1, type_probs=(0.8,0.1,0.1),shift_max=2.0,swap_max=2):\n",
    "    # Flattening our array\n",
    "    wts = x.flatten()\n",
    "    # Selecting which neurons get mutated\n",
    "    to_mutate = np.random.choice((0,1),size=wts.shape[0],p=(1-mutation_prob,mutation_prob))\n",
    "    to_mutate = np.array(np.where(to_mutate==1))\n",
    "    # Selecting which type of mutation to apply to each neuron\n",
    "    if len(np.where(to_mutate==1))>0:\n",
    "        mutation_type=np.random.choice(('shift','sign','swap'),size=to_mutate.shape[0],replace=True,p=type_probs)\n",
    "        # Performing shift mutations\n",
    "        to_shift = np.where(mutation_type=='shift')\n",
    "        wts[to_shift] = np.multiply(wts[to_shift],np.random.uniform(low=0.0,high=shift_max,size=len(to_shift)))\n",
    "        # Performing sign mutations\n",
    "        to_sign = np.where(mutation_type=='sign')\n",
    "        wts[to_sign] = wts[to_sign]*-1\n",
    "        # Performing swap mutations\n",
    "        to_swap = np.where(mutation_type=='swap')\n",
    "        wts[to_swap] = np.random.uniform(low=-1*swap_max,high=swap_max,size=len(to_swap))\n",
    "    return wts.reshape(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function that mutates an entire network\n",
    "def mutate_network(network, mutation_prob=0.1, type_probs=(0.8,0.1,0.1),shift_max=2.0,swap_max=2):\n",
    "    network = [mutate_layer(i,mutation_prob, type_probs,shift_max,swap_max) for i in network]\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our die-off function\n",
    "def die_off(pops, scores, rate=0.5):\n",
    "    # Sorting our populations first\n",
    "    sorted_inds = scores.argsort()\n",
    "    sorted_scores = -np.sort(-scores)\n",
    "    sorted_pops = pops[sorted_inds[::-1]]\n",
    "    # Killing off the weak\n",
    "    surviving_pop = sorted_pops[0:int(np.ceil(sorted_pops.shape[0]*(rate-1.)))]\n",
    "    surviving_scores = sorted_scores[0:int(np.ceil(sorted_pops.shape[0]*(rate-1.)))]\n",
    "    return surviving_pop, surviving_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for creating a child population based on fitness of parents\n",
    "def mate(pops, scores, num_children, fit_preference=2, mutation_prob=0.1, type_probs=(0.8,0.1,0.1),shift_max=2.0,swap_max=2):\n",
    "    # Creating standardized scores to use as mating probabilities\n",
    "    fitness = np.power(scores, fit_preference)\n",
    "    probs = fitness/fitness.sum()\n",
    "    # Selecting two sets of parents\n",
    "    parent1 = np.random.choice(a=range(pops.shape[0]), size=num_children, replace=True, p=probs)\n",
    "    parent2 = np.random.choice(a=range(pops.shape[0]), size=num_children, replace=True, p=probs)\n",
    "    parents = [(parent1[i], parent2[i]) for i in range(parent1.shape[0])]\n",
    "    # Next we need to create the list of the children\n",
    "    children = [offspring_network(pops[parents[i][0]], pops[parents[i][1]]) for i in range(len(parents))]\n",
    "    # Time to mutate the children\n",
    "    children = [mutate_network(i, mutation_prob, type_probs,shift_max,swap_max) for i in children]\n",
    "    children = np.array(children)\n",
    "    return np.concatenate([pops, children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our accuracy measure\n",
    "def accuracy(actual, preds):\n",
    "    return np.mean(np.where(actual==preds,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should use cross-entropy instead of accuracy for a better signal\n",
    "def cross_entropy(actual, preds):\n",
    "    return = -np.sum(actual*np.log(preds))/preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for evaluating the fitness of our models\n",
    "def evaluate_fitness(networks, X, y):\n",
    "    # Creating a list comprehension of score evaluation\n",
    "    scores = np.array([cross_entropy(y, feed_forward(X,i)) for i in networks])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST data\n",
    "X_train = np.genfromtxt('X_train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('X_test.csv', delimiter=',')\n",
    "y_train = np.genfromtxt('y_train.csv', delimiter=',')\n",
    "y_test = np.genfromtxt('y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a function now to perform our genetic modeling\n",
    "# Setting the population size\n",
    "pop_size = 50\n",
    "\n",
    "# Setting the die-off rate\n",
    "die_off_rate = 0.25\n",
    "\n",
    "# Setting our fitness preference rate\n",
    "fitness_pref = 1.0\n",
    "\n",
    "# Setting the max number of iterations\n",
    "max_iter = 500\n",
    "\n",
    "# Setting mutation rates\n",
    "mutation_rate = 0.2\n",
    "type_probs = (0.8,0.1,0.1)\n",
    "shift_max = 3.0\n",
    "swap_max = 3\n",
    "\n",
    "# Setting caps for mutation\n",
    "mutation_rate_cap = 0.5\n",
    "\n",
    "# How often do we want to display progress?\n",
    "show_every = 1\n",
    "\n",
    "# How much should we increase mutation by when not improving?\n",
    "rate_increase = 0.0125\n",
    "current_mutation_rate = mutation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 | Best Score: 0.2064 | Mutation Rate: 0.2\n",
      "Generation 2 | Best Score: 0.2064 | Mutation Rate: 0.213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-6adbe08f161c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Evaluate fitness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_fitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Initializing the previous score variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-7f63f42f75b3>\u001b[0m in \u001b[0;36mevaluate_fitness\u001b[1;34m(networks, X, y)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_fitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Creating a list comprehension of score evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-7f63f42f75b3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_fitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Creating a list comprehension of score evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a6faa95bef66>\u001b[0m in \u001b[0;36mfeed_forward\u001b[1;34m(inputs, network)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# First we need to propogate inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Now we need to iterate through all of the remaining elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating our initial set of models\n",
    "models = np.array([create_network(n_units=(32,),input_shape=784,output_shape=10) for _ in range(pop_size)])\n",
    "\n",
    "# Starting our loop to train models\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    # Evaluate fitness\n",
    "    scores = evaluate_fitness(models, X_train, y_train)\n",
    "    \n",
    "    # Initializing the previous score variable\n",
    "    if i==0:\n",
    "        previous_score = scores.max().round(4)\n",
    "    \n",
    "    # Cause die-off\n",
    "    models, scores = die_off(models, scores, die_off_rate)\n",
    "    \n",
    "    # Increasing mutation rate if necessary\n",
    "    if scores.max().round(4) == previous_score and i != 0:\n",
    "        current_mutation_rate += rate_increase\n",
    "    else:\n",
    "        current_mutation_rate = mutation_rate\n",
    "    # Checking if we have hit mutation caps\n",
    "    if current_mutation_rate > mutation_rate_cap:\n",
    "        current_mutation_rate = mutation_rate_cap\n",
    "    previous_score = scores.max().round(4)\n",
    "    \n",
    "    # Report best score\n",
    "    if (i+1)%show_every==0:\n",
    "        print('Generation',i+1,'| Best Score:',scores.max().round(4),'| Mutation Rate:',round(current_mutation_rate,3))\n",
    "    \n",
    "    # Create children\n",
    "    models = mate(pops=models,\n",
    "                  scores=scores,\n",
    "                  num_children=pop_size-models.shape[0],\n",
    "                  fit_preference=fitness_pref,\n",
    "                  mutation_prob=current_mutation_rate, \n",
    "                  type_probs=type_probs,\n",
    "                  shift_max=shift_max,\n",
    "                  swap_max=swap_max)\n",
    "\n",
    "# Done with training!\n",
    "print('Done with training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our GeneticMLP class\n",
    "class GeneticMLP():\n",
    "    def __init__(self, pop_size=100, die_off_rate=0.5,fitness_pref=1.0,generations=500,mutation_rate=0.2,\n",
    "                 type_probs=(0.8,0.1,0.1),shift_max=2.0, swap_max=2.0,mutation_rate_cap=0.4,verbose=False,print_every=1,\n",
    "                mutation_rate_increase=0.025):\n",
    "        self.pop_size = pop_size\n",
    "        self.die_off_rate = die_off_rate\n",
    "        self.fitness_pref = fitness_pref\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.type_probs = type_probs\n",
    "        self.shift_max = shift_max\n",
    "        self.swap_max = swap_max\n",
    "        self.mutation_rate_cap = mutation_rate_cap\n",
    "        self.verbose = verbose\n",
    "        self.print_every = print_every\n",
    "        self.mutation_rate_increase = mutation_rate_increase\n",
    "        return self\n",
    "\n",
    "    def fit(X, y):\n",
    "        \n",
    "    def predict(X):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test to make sure the feed forward command is working using keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Creating our network\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, input_shape=(784,),activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,epochs=5)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing model weights\n",
    "def convert_to_numpy(x):\n",
    "    weights = x[::2]\n",
    "    biases = x[1::2]\n",
    "    return np.array(weights+biases)\n",
    "keras_model = convert_to_numpy(model.get_weights())\n",
    "print(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running it though our propgation function\n",
    "print(feed_forward(X_train, keras_model))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Do\n",
    "# Add categorical cross-entropy instead of accuracy\n",
    "# Add support for accuracy as a metric\n",
    "# Create the GeneticMLP class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
