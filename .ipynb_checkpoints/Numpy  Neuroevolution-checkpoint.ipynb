{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Zero Gradient Challenge: Neuroevolution using only Numpy!\n",
    "#### By Jacob Gursky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "What if I told you that you can train neural networks without ever calculating a gradient, and only using the forward pass?  Such is the magic of **neuroevolution!** Also, I am going to show that all this can easily be done using only Numpy!  This is a bit of an ongoing project that I have been working on off and on for a while now, but let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neuroevolution?\n",
    "\n",
    "First off, for those of you that don't already know, **neuroevolution** describes the application of evolutionary algorithms to training neural networks as a gradient-free alternative!  We are going to use an extremely simple case of neuroevolution here, only using a fixed topology network and focusing on optimizing only weights and biases.  The neuroevolutionary process can be defined by four fundamental steps that are repeated until convergence is reached, starting with a pool of randomly generated networks.\n",
    "\n",
    "1. Evaluate fitness of the population\n",
    "2. Select the most fit individuals to reproduce\n",
    "3. Create offspring using crossover\n",
    "4. Introduce random mutations to children\n",
    "\n",
    "Wow, this seems pretty simple!  Let's break down some of the terminology a bit:\n",
    "\n",
    "- **Fitness**: This simply describes how well the network performed at a certain task and allows us to determine which networks to breed.  Note that because evolutionary algorithms are a form of non-convex optimization, and therefore can be used with any loss function, regardless of its differentiability (of lack thereof)\n",
    "\n",
    "- **Crossover**: This denotes the process by which a child network is created from two parent networks.  Essentially, neurons are randomly sampled from each of the parent networks and placed into the child network.  Note that we are using the entire neuron for crossover. That means that for a given neuron in a child network, every weight and the bias for that one neuron came entirely from one of the parents.\n",
    "\n",
    "- **Mutation**:  This one is probably the easiest!  In order for our child networks to improve, we have to introduce random changes to the network weights, often drawn from a uniform or normal distibution. There can be many different forms of mutation, shift mutations (which multiply the paramters by a random number), swap mutations (which replace the parameter with a random number), and sign mutations (which change the sign of a parameter) are the three that we are going to deal with in this project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Neuroevolution\n",
    "\n",
    "We should also consider the theoretical advantages of neuroevolutionary modeling.  First off, we only need to use the forward pass of the network as we only need to calculate the loss in order to determine which networks to breed.  The implications of this is obvious, the backwards pass is usually the most expensive!  Second, evolutionary algorithms are guarenteed to find the global minimum of a loss surface given enough iterations, whereas convex gradient-based methods can get stuck in local minima.  Lastly, more sophisticated forms of neuroevolution allow us to not only optimize the weights of a network, but also the structure itself!\n",
    "\n",
    "Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries\n",
    "\n",
    "As laid out in the introduction, we are going to try and use only numpy for this project, only defining the helper functions that we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "We first are going to define a few of the helper functions to set up our networks.  First off is the relu activation function, which we will use as the activation function for our hidden layers, and the softmax function for the output of the network to get probabilistic estimates of the network output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our activation function\n",
    "def relu(x):\n",
    "    return np.where(x>0,x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the softmax function\n",
    "def softmax(x):\n",
    "    x = np.exp(x - np.max(x))\n",
    "    return np.array(x / x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Random Network\n",
    "\n",
    "Another key function we need to define is a function to create a random network of a fixed topology to create our initial population of networks.  Essentially we just need to create some randomly initialized matrices according to the network structure given to the functions, so this is relatively simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our random network generation function\n",
    "def create_network(n_units=(128,64),input_shape=784,output_shape=10):\n",
    "    # First we need to randomly initialize our weight and bias matrices\n",
    "    weights = []\n",
    "    biases = []\n",
    "    # Creating the weights for the hidden layers\n",
    "    for i in range(len(n_units)):\n",
    "        if i==0:\n",
    "            weights.append(np.random.uniform(-0.15,0.15,size=(input_shape,n_units[0])).astype('float32'))\n",
    "            biases.append(np.zeros(n_units[0]).astype('float32'))\n",
    "        else:\n",
    "            weights.append(np.random.uniform(-0.15,0.15,size=(n_units[i-1],n_units[i])).astype('float32'))\n",
    "            biases.append(np.zeros(n_units[i]).astype('float32'))\n",
    "    # Creating weights and biases for output layer\n",
    "    weights.append(np.random.uniform(-0.15,0.15,size=(n_units[-1],output_shape)).astype('float32'))\n",
    "    biases.append(np.zeros(output_shape))\n",
    "    return weights+biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding the Data Forwards\n",
    "\n",
    "We also need a function that, given the weight and bias matrices of a network, can propogate given inputs forward!  This follows standard convention for perceptrons, returning probability estimates that we will use to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create our feed forward function\n",
    "def feed_forward(inputs, network):\n",
    "    # Dividing into the weights and biases\n",
    "    weights = network[0:len(network)//2]\n",
    "    biases = network[len(network)//2:]\n",
    "    # First we need to propogate inputs\n",
    "    a = relu((inputs@weights[0])+biases[0])\n",
    "    # Now we need to iterate through all of the remaining elements\n",
    "    for i in range(1,len(weights)):\n",
    "        a = relu((a@weights[i])+biases[i])\n",
    "    # Now we need to run softmax over the result\n",
    "    probs = np.apply_along_axis(softmax, axis=1, arr=a)\n",
    "    # Finally, return the max\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Offs\n",
    "Now we cannot keep all networks in consideration unfortunately, so we need to eliminate those that do not meet a certain threshold.  This is a tricky parameter to get right, as it represents the trade-off between fitness and diversity.  If we set our die off threshold to low, too many low performing networks will remain and the resulting children will be low-performing.  On the other hand, if we set this value too high, we will kill off most networks and the next generation will be nearly homogenous.  Here we are using a rate of 50% for die-off, meaning that the bottom half of performing networks are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our die-off function\n",
    "def die_off(pops, scores, rate=0.5):\n",
    "    # Sorting our populations first\n",
    "    sorted_inds = scores.argsort()\n",
    "    sorted_scores = -np.sort(-scores)\n",
    "    sorted_pops = pops[sorted_inds[::-1]]\n",
    "    # Killing off the weak\n",
    "    surviving_pop = sorted_pops[0:int(np.ceil(sorted_pops.shape[0]*(rate-1.)))]\n",
    "    surviving_scores = sorted_scores[0:int(np.ceil(sorted_pops.shape[0]*(rate-1.)))]\n",
    "    return surviving_pop, surviving_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Offspring Networks\n",
    "\n",
    "Another key aspect of neuroevolution is being able to have networks breed!  Therefore, we need a function that, given two networks, will create a child network that has half of the neurons from the two parent networks!  Again, relatively simple code that should be easy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a crossover function that can handle a uniform probability\n",
    "def crossover_networks(weight_set1, weight_set2, max_crossover=0.5, crossover_prob=1.0):\n",
    "    # Drawing a random number first to determine if crossover happens\n",
    "    random = np.random.uniform(0.0,1.0,1)[0]\n",
    "    if random <= crossover_prob:\n",
    "        # Lets grab weight and bias matrices\n",
    "        weights1, weights2 = weight_set1[:len(weight_set1)//2], weight_set2[:len(weight_set1)//2]\n",
    "        bias1, bias2 = weight_set1[len(weight_set1)//2:], weight_set2[len(weight_set2)//2:]\n",
    "        # Getting the probability of cross-over\n",
    "        cross_prob = np.random.uniform(low=0.0,high=max_crossover,size=1)[0]\n",
    "        selected_neurons = np.array([np.random.choice(range(i.shape[0]), size=int(i.shape[0]*cross_prob)) for i in bias1])\n",
    "        # Grabbing the child weight_matrices\n",
    "        child_weights = [i.copy() for i in weights1]\n",
    "        child_biases = [i.copy() for i in bias1]\n",
    "        # Looping over selected neurons\n",
    "        for i in range(selected_neurons.shape[0]):\n",
    "            child_weights[i][:,tuple(selected_neurons[i])] = weights2[i][:,tuple(selected_neurons[i])]\n",
    "            child_biases[i][selected_neurons[i]] = bias2[i][selected_neurons[i]]\n",
    "        return child_weights+child_biases\n",
    "    else:\n",
    "        return weight_set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Mutations\n",
    "\n",
    "As mentioned above, we are interested in three main types of mutations: shift, scale, and sign mutations.  These will be what allow our networks to learn, in addition to utilizing cross-over.  First we break the mutation up into two probabilities, the first being whether or not a neuron will actually be mutated, and if so, the probabilities associated with each type of mutation.  Lastly, we also have some parameters denoting the maximum absolute values of the shift and swap mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to mutate a single network\n",
    "def mutate_layer(x, mutation_prob=0.1, type_probs=(0.8,0.1,0.1),shift_max=2.0,swap_max=2):\n",
    "    # Flattening our array\n",
    "    wts = x.flatten()\n",
    "    # Selecting which neurons get mutated\n",
    "    to_mutate = np.random.choice((0,1),size=wts.shape[0],p=(1-mutation_prob,mutation_prob))\n",
    "    to_mutate = np.array(np.where(to_mutate==1))\n",
    "    # Selecting which type of mutation to apply to each neuron\n",
    "    if len(np.where(to_mutate==1))>0:\n",
    "        mutation_type=np.random.choice(('shift','sign','swap'),size=to_mutate.shape[0],replace=True,p=type_probs)\n",
    "        # Performing shift mutations\n",
    "        to_shift = np.where(mutation_type=='shift')\n",
    "        wts[to_shift] = np.multiply(wts[to_shift],np.random.uniform(low=0.0,high=shift_max,size=len(to_shift)))\n",
    "        # Performing sign mutations\n",
    "        to_sign = np.where(mutation_type=='sign')\n",
    "        wts[to_sign] = wts[to_sign]*-1\n",
    "        # Performing swap mutations\n",
    "        to_swap = np.where(mutation_type=='swap')\n",
    "        wts[to_swap] = np.random.uniform(low=-1*swap_max,high=swap_max,size=len(to_swap))\n",
    "    return wts.reshape(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function that mutates an entire network\n",
    "def mutate_network(network, mutation_prob=0.1, type_probs=(0.8,0.1,0.1),shift_max=2.0,swap_max=2):\n",
    "    network = [mutate_layer(i,mutation_prob, type_probs,shift_max,swap_max) for i in network]\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mating\n",
    "After the die-off, we need to replenish the population with new individuals that are cross-over networks of two parent networks.  The function defined below controls both the creation of new networks via cross-over and the mutation of the children.  We are also interested in an additional hyperparameter when selecting parents: fitness preference.  We want to have some sort of weighted probabilities when selecting parents (more fit parents should have more children).  To get the probability a parent will be sampled, we use the following equation:\n",
    "\n",
    "$$P_i=\\frac{F_i^{m}}{\\sum_{n=1}^{k}F_n^{m}}$$\n",
    "\n",
    "Where $P_i$ is the probability network $i$ will be a parent, $F_i$ represents the fitness of network $i$, $m$ represents the fitness preference for mating, $k$ is the number of networks in the population.  Note that when $m$ is equal to zero, parents are randomly selected as mating probabilities become uniform, and $m$ equals infinity represents only the most fit network being able to mate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for creating a child population based on fitness of parents\n",
    "def mate(pops, scores, num_children, fit_preference=2, mutation_prob=0.1, type_probs=(0.8,0.1,0.1),\n",
    "         shift_max=2.0, swap_max=2, max_crossover=0.5, crossover_prob=1.0):\n",
    "    # Creating standardized scores to use as mating probabilities\n",
    "    fitness = np.power(scores, fit_preference)\n",
    "    probs = fitness/fitness.sum()\n",
    "    # Selecting two sets of parents\n",
    "    parent1 = np.random.choice(a=range(pops.shape[0]), size=num_children, replace=True, p=probs)\n",
    "    parent2 = np.random.choice(a=range(pops.shape[0]), size=num_children, replace=True, p=probs)\n",
    "    parents = [(parent1[i], parent2[i]) for i in range(parent1.shape[0])]\n",
    "    # Next we need to create the list of the children\n",
    "    children = [crossover_networks(pops[parents[i][0]], pops[parents[i][1]], max_crossover, crossover_prob) for i in range(len(parents))]\n",
    "    # Time to mutate the children\n",
    "    children = [mutate_network(i, mutation_prob, type_probs,shift_max,swap_max) for i in children]\n",
    "    children = np.array(children)\n",
    "    return np.concatenate([pops, children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Fitness Function and Other Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a one-hot encoding function\n",
    "def one_hot(targets):\n",
    "    y = np.zeros((targets.shape[0], len(np.unique(targets))))\n",
    "    y[np.arange(targets.shape[0]), targets] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our accuracy measure\n",
    "def accuracy(actual, preds):\n",
    "    return np.mean(np.where(actual==np.argmax(preds,axis=1),1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should use cross-entropy instead of accuracy for a better signal\n",
    "def cross_entropy(actual, preds):\n",
    "    return -np.sum(one_hot(actual)*np.log(preds))/preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for evaluating the fitness of our models\n",
    "def evaluate_fitness(preds, X, y):\n",
    "    # Creating a list comprehension of score evaluation using inverse cross-entropy\n",
    "    scores = np.array([1/cross_entropy(y, i) for i in preds])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking accuracy of our networks\n",
    "def evaluate_accuracy(preds, X, y):\n",
    "    scores = np.array([accuracy(y, i) for i in preds])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets write a function to get the average class/observation variance across a population of models\n",
    "def prediction_variance(preds):\n",
    "    pred_var = preds.var().mean().round(8)\n",
    "    return pred_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST data\n",
    "X_train = np.genfromtxt('X_train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('X_test.csv', delimiter=',')\n",
    "y_train = np.genfromtxt('y_train.csv', delimiter=',')\n",
    "y_test = np.genfromtxt('y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our GeneticMLP class\n",
    "class GeneticMLP():\n",
    "    def __init__(self, architecture = (16,), input_shape=784, output_shape=10, pop_size=100, die_off_rate=0.5,fitness_pref=1.0,\n",
    "                 generations=100,mutation_rate=0.1,type_probs=(0.8,0.1,0.1),shift_max=2.0, swap_max=2.0,mutation_rate_cap=0.25,\n",
    "                 verbose=False,print_every=1, mutation_rate_increase=0.025, max_crossover=0.5, crossover_prob=1.0):\n",
    "        self.architecture = architecture\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.pop_size = pop_size\n",
    "        self.die_off_rate = die_off_rate\n",
    "        self.fitness_pref = fitness_pref\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.type_probs = type_probs\n",
    "        self.shift_max = shift_max\n",
    "        self.swap_max = swap_max\n",
    "        self.mutation_rate_cap = mutation_rate_cap\n",
    "        self.verbose = verbose\n",
    "        self.print_every = print_every\n",
    "        self.mutation_rate_increase = mutation_rate_increase\n",
    "        self.max_crossover = max_crossover\n",
    "        self.crossover_prob = crossover_prob\n",
    "        self.best_ce = []\n",
    "        self.best_accuracy = []\n",
    "        self.pred_var = []\n",
    "        self.ce_var = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Creating our initial set of models\n",
    "        models = np.array([create_network(n_units=self.architecture,\n",
    "                                          input_shape=self.input_shape,\n",
    "                                          output_shape=self.output_shape) for _ in range(self.pop_size)])\n",
    "\n",
    "        # Starting our loop to train models\n",
    "        for i in range(self.generations):\n",
    "\n",
    "            # Evaluate fitness\n",
    "            # Getting predictions\n",
    "            preds = [feed_forward(X,i) for i in models]\n",
    "            \n",
    "            # Calculating cross-entropy\n",
    "            scores = evaluate_fitness(preds, X, y)\n",
    "            \n",
    "            # Calculating accuracy to track\n",
    "            track_scores = evaluate_accuracy(preds, X, y)\n",
    "            \n",
    "            # Getting the probability estimate variance\n",
    "            variance = prediction_variance(np.array(preds))\n",
    "            \n",
    "            # We also need to get the variance of the performance of the population\n",
    "            ce_var = scores.var()\n",
    "\n",
    "            # Appending to our lists tracked scores\n",
    "            self.best_ce.append(scores.min())\n",
    "            self.best_accuracy.append(track_scores.min())\n",
    "            self.pred_var.append(variance)\n",
    "            self.ce_var.append(ce_var)\n",
    "\n",
    "            # Initializing the previous score variable\n",
    "            if i==0:\n",
    "                previous_score = scores.max().round(4)\n",
    "\n",
    "            # Cause die-off\n",
    "            models, scores = die_off(models, scores, self.die_off_rate)\n",
    "\n",
    "            # Increasing mutation rate if necessary\n",
    "            if scores.max() == previous_score and i != 0:\n",
    "                current_mutation_rate += self.mutation_rate_increase\n",
    "            else:\n",
    "                current_mutation_rate = self.mutation_rate\n",
    "            # Checking if we have hit mutation caps\n",
    "            if current_mutation_rate > self.mutation_rate_cap:\n",
    "                current_mutation_rate = self.mutation_rate_cap\n",
    "            previous_score = scores.max().round(4)\n",
    "\n",
    "            # Report best score\n",
    "            if (i+1)%self.print_every==0 and self.verbose is True:\n",
    "                print('Generation',i+1,'| Best Accuracy:',track_scores.max().round(4),' | Best CE:',round(1/scores.min(),4),'| Prediction Variance:', variance, '| CE Variance:',ce_var)\n",
    "\n",
    "            # Create children\n",
    "            models = mate(pops=models,\n",
    "                          scores=scores,\n",
    "                          num_children=self.pop_size-models.shape[0],\n",
    "                          fit_preference=self.fitness_pref,\n",
    "                          mutation_prob=current_mutation_rate, \n",
    "                          type_probs=self.type_probs,\n",
    "                          shift_max=self.shift_max,\n",
    "                          swap_max=self.swap_max,\n",
    "                          max_crossover=self.max_crossover,\n",
    "                          crossover_prob=self.crossover_prob)\n",
    "\n",
    "        # Done with training!\n",
    "        print('Done with training!')\n",
    "        \n",
    "        # Finding the best model\n",
    "        self.model = models[np.argmax(scores)]\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Performing prediction\n",
    "        prob_preds = feed_forward(X, self.model)\n",
    "        \n",
    "        # Getting the most probable value\n",
    "        preds = np.argmax(prob_preds,axis=1)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our object\n",
    "genetic_mlp = GeneticMLP(architecture=(8,),\n",
    "                         pop_size=1000,\n",
    "                         generations=100, \n",
    "                         verbose=True, \n",
    "                         max_crossover=0.3, \n",
    "                         crossover_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 | Best Accuracy: 0.196  | Best CE: 2.3056 | Prediction Variance: 6.741e-05 | CE Variance: 2.1805179563715515e-06\n",
      "Generation 2 | Best Accuracy: 0.196  | Best CE: 2.3014 | Prediction Variance: 0.00018384 | CE Variance: 1.4891566221676312e-05\n",
      "Generation 3 | Best Accuracy: 0.1961  | Best CE: 2.2981 | Prediction Variance: 0.00017684 | CE Variance: 1.5037157199384752e-05\n",
      "Generation 4 | Best Accuracy: 0.2002  | Best CE: 2.2951 | Prediction Variance: 0.00025745 | CE Variance: 3.706135433103192e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8fa132495d26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training our model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgenetic_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-0bd7a7ff5c61>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# Evaluate fitness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m# Getting predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# Calculating cross-entropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-0bd7a7ff5c61>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# Evaluate fitness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m# Getting predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# Calculating cross-entropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-cfb86def5b30>\u001b[0m in \u001b[0;36mfeed_forward\u001b[1;34m(inputs, network)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Now we need to run softmax over the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Finally, return the max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mbuff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-664ab805859d>\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Defining the softmax function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2503\u001b[0m     \"\"\"\n\u001b[0;32m   2504\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[1;32m-> 2505\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training our model\n",
    "genetic_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting test predictions\n",
    "test_preds = genetic_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2763\n"
     ]
    }
   ],
   "source": [
    "# Finding the accuracy\n",
    "print((test_preds==y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jacob\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 1.0138 - acc: 0.6873\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.6395 - acc: 0.8145\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.5899 - acc: 0.8308\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.5647 - acc: 0.8383\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.5485 - acc: 0.8433\n",
      "10000/10000 [==============================] - 1s 50us/sample - loss: 0.5439 - acc: 0.8415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.543911269569397, 0.8415]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test to make sure the feed forward command is working using keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Creating our network\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(4, input_shape=(784,),activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,epochs=5)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing model weights\n",
    "def convert_to_numpy(x):\n",
    "    weights = x[::2]\n",
    "    biases = x[1::2]\n",
    "    return np.array(weights+biases)\n",
    "keras_model = convert_to_numpy(model.get_weights())\n",
    "print(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.01415443, -0.04415147, -0.06734966],\n",
      "       [-0.0305978 ,  0.12892628,  0.06926978],\n",
      "       [-0.07606962,  0.03873779, -0.10863217],\n",
      "       [-0.13481519, -0.01681741,  0.06986276]], dtype=float32), array([[-0.02708984, -0.01067051],\n",
      "       [-0.14952284,  0.0231384 ],\n",
      "       [-0.07671878, -0.0440743 ]], dtype=float32), array([0., 0., 0.], dtype=float32), array([0., 0.])]\n",
      "[array([[-0.14336349,  0.12438363,  0.12234331],\n",
      "       [ 0.14879163, -0.07256004, -0.01450817],\n",
      "       [-0.07572136,  0.08463401,  0.04548424],\n",
      "       [-0.13866317,  0.1350255 ,  0.11784375]], dtype=float32), array([[-0.0251986 , -0.0843462 ],\n",
      "       [-0.03059931,  0.0875032 ],\n",
      "       [ 0.06256985,  0.08612204]], dtype=float32), array([0., 0., 0.], dtype=float32), array([0., 0.])]\n",
      "[array([[-0.01415443, -0.04415147,  0.12234331],\n",
      "       [-0.0305978 ,  0.12892628, -0.01450817],\n",
      "       [-0.07606962,  0.03873779,  0.04548424],\n",
      "       [-0.13481519, -0.01681741,  0.11784375]], dtype=float32), array([[-0.0251986 , -0.01067051],\n",
      "       [-0.03059931,  0.0231384 ],\n",
      "       [ 0.06256985, -0.0440743 ]], dtype=float32), array([0., 0., 0.], dtype=float32), array([0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# Time to test the new crossover function\n",
    "parent1 = create_network(n_units=(3,),input_shape=4,output_shape=2)\n",
    "parent2 = create_network(n_units=(3,),input_shape=4,output_shape=2)\n",
    "child = crossover_networks(parent1, parent2, max_crossover=1.0)\n",
    "print(parent1)\n",
    "print(parent2)\n",
    "print(child)\n",
    "# Okay, it seems to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333333\n",
      "  0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      "  0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.98431373\n",
      "  0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.77647059 0.71372549 0.96862745 0.94509804\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      "  0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.05490196 0.00392157 0.60392157\n",
      "  0.99215686 0.35294118 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.54509804\n",
      "  0.99215686 0.74509804 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04313725\n",
      "  0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.1372549  0.94509804 0.88235294 0.62745098 0.42352941 0.00392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31764706 0.94117647 0.99215686 0.99215686 0.46666667\n",
      "  0.09803922 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      "  0.58823529 0.10588235 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.0627451  0.36470588 0.98823529\n",
      "  0.99215686 0.73333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.97647059\n",
      "  0.99215686 0.97647059 0.25098039 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      "  0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15294118 0.58039216 0.89803922 0.99215686 0.99215686 0.99215686\n",
      "  0.98039216 0.71372549 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.09411765 0.44705882\n",
      "  0.86666667 0.99215686 0.99215686 0.99215686 0.99215686 0.78823529\n",
      "  0.30588235 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.76470588 0.31372549 0.03529412 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.21568627 0.6745098\n",
      "  0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
      "  0.52156863 0.04313725 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.53333333 0.99215686\n",
      "  0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# We need to write a function to downsample our MNIST data\n",
    "# First we need to reshape so that each image is a dimension\n",
    "X_train.shape\n",
    "print(X_train.reshape(60000, 28, 28).shape)\n",
    "print(X_train.reshape(60000, 28, 28)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
